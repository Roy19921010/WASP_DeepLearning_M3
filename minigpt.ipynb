{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb691401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 8.6743\n",
      "Epoch 2, Loss: 5.8412\n",
      "Epoch 3, Loss: 4.6374\n",
      "Epoch 4, Loss: 3.9575\n",
      "Epoch 5, Loss: 3.4134\n",
      "Epoch 6, Loss: 2.8922\n",
      "Epoch 7, Loss: 2.3803\n",
      "Epoch 8, Loss: 1.9069\n",
      "Epoch 9, Loss: 1.4812\n",
      "Epoch 10, Loss: 1.1083\n",
      "Epoch 11, Loss: 0.8145\n",
      "Epoch 12, Loss: 0.5938\n",
      "Epoch 13, Loss: 0.4387\n",
      "Epoch 14, Loss: 0.3311\n",
      "Epoch 15, Loss: 0.2561\n",
      "Epoch 16, Loss: 0.2033\n",
      "Epoch 17, Loss: 0.1676\n",
      "Epoch 18, Loss: 0.1398\n",
      "Epoch 19, Loss: 0.1204\n",
      "Epoch 20, Loss: 0.1039\n",
      "Epoch 21, Loss: 0.0917\n",
      "Epoch 22, Loss: 0.0819\n",
      "Epoch 23, Loss: 0.0728\n",
      "Epoch 24, Loss: 0.0667\n",
      "Epoch 25, Loss: 0.0605\n",
      "Epoch 26, Loss: 0.0553\n",
      "Epoch 27, Loss: 0.0510\n",
      "Epoch 28, Loss: 0.0480\n",
      "Epoch 29, Loss: 0.0445\n",
      "Epoch 30, Loss: 0.0414\n",
      "Epoch 31, Loss: 0.0386\n",
      "Epoch 32, Loss: 0.0365\n",
      "Epoch 33, Loss: 0.0344\n",
      "Epoch 34, Loss: 0.0326\n",
      "Epoch 35, Loss: 0.0307\n",
      "Epoch 36, Loss: 0.0293\n",
      "Epoch 37, Loss: 0.0280\n",
      "Epoch 38, Loss: 0.0265\n",
      "Epoch 39, Loss: 0.0255\n",
      "Epoch 40, Loss: 0.0244\n",
      "\n",
      "Generated text:\n",
      "Roy has hobbies like tennis, guitar and video games. and\n",
      "\n",
      "Generated text:\n",
      "Roy works in Ericsson and Chalmers. and Chalmers\n",
      "\n",
      "Generated text:\n",
      "Introduce Roy: hobbies like tennis, guitar and video games. and\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import random\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, d_model=128, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        tok_emb = self.token_emb(x)             # (B, T, d_model)\n",
    "        x = tok_emb + self.pos_emb[:, :T, :]    # add positional embedding\n",
    "        x = x.transpose(0, 1)                   # (T, B, d_model) for transformer\n",
    "        #mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)  # shape (T, T), adding causal attention mask\n",
    "        x = self.transformer(x,mask=None)                 # (T, B, d_model)\n",
    "        x = x.transpose(0, 1)                   # (B, T, d_model)\n",
    "        logits = self.lm_head(x)                # (B, T, vocab_size)\n",
    "        return logits\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=128):\n",
    "        self.examples = []\n",
    "        for text in texts:\n",
    "            tokens = tokenizer.encode(text)\n",
    "            for i in range(0, len(tokens) - seq_len):\n",
    "                input_seq = tokens[i:i+seq_len]\n",
    "                target_seq = tokens[i+1:i+seq_len+1]\n",
    "                self.examples.append((input_seq, target_seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.examples[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt')['input_ids'].to(next(model.parameters()).device)\n",
    "    generated = input_ids\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            #next_token = torch.multinomial(probs, num_samples=1) #randomness\n",
    "            next_token = torch.argmax(probs, dim=-1, keepdim=True) #deterministic\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return tokenizer.decode(generated[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Dummy text data\n",
    "    texts = [\"Roy lives in Sweden.\", \"Roy is PhD student.\", \"Roy works in Ericsson and Chalmers.\", \"Roy has hobbies like tennis, guitar and video games.\"] * 20\n",
    "\n",
    "    # Parameters\n",
    "    seq_len = 8\n",
    "    batch_size = 8\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    # DataLoader\n",
    "    dataset = ToyDataset(texts, tokenizer, seq_len=seq_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Model & Optimizer\n",
    "    model = MiniGPT(vocab_size=vocab_size, seq_len=64).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(40):\n",
    "        loss = train(model, dataloader, optimizer, device)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "    prompt = \"Roy has\"\n",
    "    output = generate(model, tokenizer, prompt, max_new_tokens=10)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(output)\n",
    "\n",
    "    prompt = \"Roy works\"\n",
    "    output = generate(model, tokenizer, prompt, max_new_tokens=10)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(output)\n",
    "\n",
    "    prompt = \"Introduce Roy:\"\n",
    "    output = generate(model, tokenizer, prompt, max_new_tokens=10)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
