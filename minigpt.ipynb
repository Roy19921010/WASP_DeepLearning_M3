{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb691401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.2804\n",
      "Epoch 2, Loss: 4.2393\n",
      "Epoch 3, Loss: 3.4378\n",
      "Epoch 4, Loss: 2.7730\n",
      "Epoch 5, Loss: 2.1170\n",
      "Epoch 6, Loss: 1.5112\n",
      "Epoch 7, Loss: 1.0032\n",
      "Epoch 8, Loss: 0.6411\n",
      "Epoch 9, Loss: 0.4131\n",
      "Epoch 10, Loss: 0.2794\n",
      "Epoch 11, Loss: 0.1999\n",
      "Epoch 12, Loss: 0.1514\n",
      "Epoch 13, Loss: 0.1206\n",
      "Epoch 14, Loss: 0.0996\n",
      "Epoch 15, Loss: 0.0840\n",
      "Epoch 16, Loss: 0.0723\n",
      "Epoch 17, Loss: 0.0636\n",
      "Epoch 18, Loss: 0.0563\n",
      "Epoch 19, Loss: 0.0500\n",
      "Epoch 20, Loss: 0.0453\n",
      "Epoch 21, Loss: 0.0410\n",
      "Epoch 22, Loss: 0.0374\n",
      "Epoch 23, Loss: 0.0345\n",
      "Epoch 24, Loss: 0.0317\n",
      "Epoch 25, Loss: 0.0294\n",
      "Epoch 26, Loss: 0.0273\n",
      "Epoch 27, Loss: 0.0257\n",
      "Epoch 28, Loss: 0.0240\n",
      "Epoch 29, Loss: 0.0227\n",
      "Epoch 30, Loss: 0.0214\n",
      "Epoch 31, Loss: 0.0201\n",
      "Epoch 32, Loss: 0.0190\n",
      "Epoch 33, Loss: 0.0181\n",
      "Epoch 34, Loss: 0.0171\n",
      "Epoch 35, Loss: 0.0163\n",
      "Epoch 36, Loss: 0.0156\n",
      "Epoch 37, Loss: 0.0148\n",
      "Epoch 38, Loss: 0.0142\n",
      "Epoch 39, Loss: 0.0136\n",
      "Epoch 40, Loss: 0.0131\n",
      "Epoch 41, Loss: 0.0126\n",
      "Epoch 42, Loss: 0.0121\n",
      "Epoch 43, Loss: 0.0117\n",
      "Epoch 44, Loss: 0.0112\n",
      "Epoch 45, Loss: 0.0108\n",
      "Epoch 46, Loss: 0.0104\n",
      "Epoch 47, Loss: 0.0101\n",
      "Epoch 48, Loss: 0.0097\n",
      "Epoch 49, Loss: 0.0094\n",
      "Epoch 50, Loss: 0.0091\n",
      "Epoch 51, Loss: 0.0088\n",
      "Epoch 52, Loss: 0.0085\n",
      "Epoch 53, Loss: 0.0083\n",
      "Epoch 54, Loss: 0.0080\n",
      "Epoch 55, Loss: 0.0078\n",
      "Epoch 56, Loss: 0.0076\n",
      "Epoch 57, Loss: 0.0074\n",
      "Epoch 58, Loss: 0.0072\n",
      "Epoch 59, Loss: 0.0069\n",
      "Epoch 60, Loss: 0.0067\n",
      "Epoch 61, Loss: 0.0066\n",
      "Epoch 62, Loss: 0.0064\n",
      "Epoch 63, Loss: 0.0063\n",
      "Epoch 64, Loss: 0.0061\n",
      "Epoch 65, Loss: 0.0059\n",
      "Epoch 66, Loss: 0.0058\n",
      "Epoch 67, Loss: 0.0057\n",
      "Epoch 68, Loss: 0.0055\n",
      "Epoch 69, Loss: 0.0054\n",
      "Epoch 70, Loss: 0.0053\n",
      "Epoch 71, Loss: 0.0052\n",
      "Epoch 72, Loss: 0.0050\n",
      "Epoch 73, Loss: 0.0049\n",
      "Epoch 74, Loss: 0.0048\n",
      "Epoch 75, Loss: 0.0047\n",
      "Epoch 76, Loss: 0.0046\n",
      "Epoch 77, Loss: 0.0045\n",
      "Epoch 78, Loss: 0.0044\n",
      "Epoch 79, Loss: 0.0043\n",
      "Epoch 80, Loss: 0.0042\n",
      "Epoch 81, Loss: 0.0041\n",
      "Epoch 82, Loss: 0.0041\n",
      "Epoch 83, Loss: 0.0040\n",
      "Epoch 84, Loss: 0.0039\n",
      "Epoch 85, Loss: 0.0038\n",
      "Epoch 86, Loss: 0.0038\n",
      "Epoch 87, Loss: 0.0037\n",
      "Epoch 88, Loss: 0.0036\n",
      "Epoch 89, Loss: 0.0036\n",
      "Epoch 90, Loss: 0.0035\n",
      "Epoch 91, Loss: 0.0034\n",
      "Epoch 92, Loss: 0.0034\n",
      "Epoch 93, Loss: 0.0033\n",
      "Epoch 94, Loss: 0.0032\n",
      "Epoch 95, Loss: 0.0032\n",
      "Epoch 96, Loss: 0.0031\n",
      "Epoch 97, Loss: 0.0031\n",
      "Epoch 98, Loss: 0.0030\n",
      "Epoch 99, Loss: 0.0029\n",
      "Epoch 100, Loss: 0.0029\n",
      "\n",
      "Generated text:\n",
      "Roy is. and Chalmers. Ericsson and Chalmers. Ericsson and Chalmers. Ericsson and Chal\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import random\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, d_model=128, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        tok_emb = self.token_emb(x)             # (B, T, d_model)\n",
    "        x = tok_emb + self.pos_emb[:, :T, :]    # add positional embedding\n",
    "        x = x.transpose(0, 1)                   # (T, B, d_model) for transformer\n",
    "        #mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)  # shape (T, T), adding causal attention mask\n",
    "        x = self.transformer(x)                 # (T, B, d_model)\n",
    "        x = x.transpose(0, 1)                   # (B, T, d_model)\n",
    "        logits = self.lm_head(x)                # (B, T, vocab_size)\n",
    "        return logits\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=128):\n",
    "        self.examples = []\n",
    "        for text in texts:\n",
    "            tokens = tokenizer.encode(text)\n",
    "            for i in range(0, len(tokens) - seq_len):\n",
    "                input_seq = tokens[i:i+seq_len]\n",
    "                target_seq = tokens[i+1:i+seq_len+1]\n",
    "                self.examples.append((input_seq, target_seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.examples[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt')['input_ids'].to(next(model.parameters()).device)\n",
    "    generated = input_ids\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            #next_token = torch.multinomial(probs, num_samples=1) #randomness\n",
    "            next_token = torch.argmax(probs, dim=-1, keepdim=True) #deterministic\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return tokenizer.decode(generated[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Dummy text data\n",
    "    #texts = [\"The sky is blue.\", \"The cat sat on the mat.\", \"The quick brown fox jumps over the lazy dog.\"] * 100\n",
    "    texts = [\"Roy lives in Sweden.\", \"Roy is a PhD student.\", \"Roy works in Ericsson and Chalmers.\"] * 100\n",
    "\n",
    "    # Parameters\n",
    "    seq_len = 8\n",
    "    batch_size = 8\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    # DataLoader\n",
    "    dataset = ToyDataset(texts, tokenizer, seq_len=seq_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Model & Optimizer\n",
    "    model = MiniGPT(vocab_size=vocab_size, seq_len=64).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(100):\n",
    "        loss = train(model, dataloader, optimizer, device)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "    # Generation Example\n",
    "    prompt = \"Roy is.\"\n",
    "    output = generate(model, tokenizer, prompt, max_new_tokens=20)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
